{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_file = './data/rt-polarity.neg'\n",
    "pos_file = './data/rt-polarity.pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 0 the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "## 0 the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\n",
      "\n",
      "\n",
      "## 1 the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "## 1 the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\n",
      "\n",
      "\n",
      "## 2 effective but too-tepid biopic\n",
      "\n",
      "## 2 effective but too tepid biopic\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(pos_file, 'r', encoding='utf8') as f:\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        print('##', i,  line)\n",
    "        print('##', i,  clean_str(line))\n",
    "        print()\n",
    "        print()\n",
    "        if i > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载数据\n",
    "import numpy as np\n",
    "\n",
    "positive_examples = list(open(pos_file, 'r', encoding='utf8').readlines())\n",
    "negative_examples = list(open(neg_file, 'r', encoding='utf8').readlines())\n",
    "\n",
    "x_text = positive_examples + negative_examples\n",
    "x_text = [clean_str(sent) for sent in x_text]\n",
    "\n",
    "# generate labels\n",
    "positive_labels = [[1, 0] for _ in positive_examples]\n",
    "negative_labels = [[0, 1] for _ in negative_examples]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\", \"the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\"]\n",
      "[[1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_text[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(y)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    \n",
    "    num_batches_of_one_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffle_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffle_data = data\n",
    "        # 取数据\n",
    "        for batch_id in range(num_batches_of_one_epoch):\n",
    "            start_index = batch_id * batch_size\n",
    "            end_index = (batch_id + 1) * batch_size\n",
    "            end_index = min(end_index, data_size)\n",
    "            yield shuffle_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "batch: [0 1 2 3 4]\n",
      "batch: [5 6 7 8 9]\n",
      "batch: [10 11 12 13 14]\n",
      "batch: [15 16 17 18 19]\n",
      "batch: [20 21]\n",
      "batch: [0 1 2 3 4]\n",
      "batch: [5 6 7 8 9]\n",
      "batch: [10 11 12 13 14]\n",
      "batch: [15 16 17 18 19]\n",
      "batch: [20 21]\n"
     ]
    }
   ],
   "source": [
    "data = np.arange(22)\n",
    "print(data)\n",
    "\n",
    "for batch in batch_iter(data, batch_size=5, num_epochs=2, shuffle=False):\n",
    "    print('batch:', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "[  0  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170\n",
      " 180 190 200 210]\n",
      "[(0, 0), (1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (6, 60), (7, 70), (8, 80), (9, 90), (10, 100), (11, 110), (12, 120), (13, 130), (14, 140), (15, 150), (16, 160), (17, 170), (18, 180), (19, 190), (20, 200), (21, 210)]\n",
      "batch 0: [[ 0  0]\n",
      " [ 1 10]\n",
      " [ 2 20]\n",
      " [ 3 30]\n",
      " [ 4 40]]\n",
      "batch 1: [[ 5 50]\n",
      " [ 6 60]\n",
      " [ 7 70]\n",
      " [ 8 80]\n",
      " [ 9 90]]\n",
      "batch 2: [[ 10 100]\n",
      " [ 11 110]\n",
      " [ 12 120]\n",
      " [ 13 130]\n",
      " [ 14 140]]\n",
      "batch 3: [[ 15 150]\n",
      " [ 16 160]\n",
      " [ 17 170]\n",
      " [ 18 180]\n",
      " [ 19 190]]\n",
      "batch 4: [[ 20 200]\n",
      " [ 21 210]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(22)\n",
    "y = x * 10\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "data = list(zip(x, y))\n",
    "print(data)\n",
    "\n",
    "for i, batch in enumerate(batch_iter(data=data, batch_size=5, num_epochs=1, shuffle=False)):\n",
    "    print('batch %d:' % i, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为模型准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_length = max(len(x.split(' ')) for x in x_text)\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  1  7  8  9 10 11 12 13 14  9 15  5 16 17 18 19 20 21\n",
      " 22 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\n",
      "[ 1 31 32 33 34  1 35 34  1 36 37  3 38 39 13 17 40 34 41 42 43 44 45 46\n",
      " 47 48 49  9 50 51 34 52 53 53 54  9 55 56  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\n",
      "[57 58 59 60 61  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "effective but too tepid biopic\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "vocab_processor\n",
    "\n",
    "# 由单词组成的文本，被转换为 由单词索引构成的列表\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "for i, word_idxs in enumerate(x[:3]):\n",
    "    print(word_idxs)\n",
    "    print(x_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger jean claud van damme or steven segal <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试由单词索引列表，再转换为文本\n",
    "# 获取单词的索引：vocab_processor.vocabulary_.get('rock')\n",
    "# 由索引获取单词：vocab_processor.vocabulary_.reverse(2)\n",
    "print(vocab_processor.vocabulary_.reverse(0))\n",
    "\n",
    "' '.join([vocab_processor.vocabulary_.reverse(word_id) for word_id in x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18758"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(x))\n",
    "len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印词表\n",
    "print({vocab_processor.vocabulary_.reverse(word_id): word_id for word_id in range(len(vocab_processor.vocabulary_))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dd': 0, 'ee': 1, 'ff': 2}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word: i for i, word in enumerate(['dd', 'ee', 'ff'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把数据切分为训练集核测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 56)\n",
      "(10662, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test split: 9596, 1066\n"
     ]
    }
   ],
   "source": [
    "# 打乱顺序\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# 划分训练集和测试集\n",
    "test_percent = 0.1\n",
    "test_num_samples = int(test_percent * len(y))\n",
    "x_train, x_test = x_shuffled[:-test_num_samples], x_shuffled[-test_num_samples:]\n",
    "y_train, y_test = y_shuffled[:-test_num_samples], y_shuffled[-test_num_samples:]\n",
    "\n",
    "print('train/test split: {}, {}'.format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "test: [0 1]\n",
      "train: [2 3 4 5 6 7 8 9]\n",
      "test: [8 9]\n",
      "train: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.arange(10)\n",
    "print(arr1)\n",
    "\n",
    "# 从前面取 test\n",
    "print('test:', arr1[:2])\n",
    "print('train:', arr1[2:])\n",
    "# 先取 train，剩余的做 test\n",
    "print('test:', arr1[-2:])\n",
    "print('train:', arr1[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  292    84   523  1889    99   100   274    67    13 15402   121  4596\n",
      "    600   722  1456  2279   944   207  8493   503   125 10507     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1    89 11697   826  1012     1  3666    34     1   511   657   146\n",
      "    483    84   249  1798    17   899  2508  4084    58    78   539    12\n",
      "    736  3530    34    17  4635     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "[[0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "  x: [array([  292,    84,   523,  1889,    99,   100,   274,    67,    13,\n",
      "       15402,   121,  4596,   600,   722,  1456,  2279,   944,   207,\n",
      "        8493,   503,   125, 10507,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)\n",
      " array([    1,    89, 11697,   826,  1012,     1,  3666,    34,     1,\n",
      "         511,   657,   146,   483,    84,   249,  1798,    17,   899,\n",
      "        2508,  4084,    58,    78,   539,    12,   736,  3530,    34,\n",
      "          17,  4635,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)]\n",
      "  y: [array([0, 1]) array([1, 0])]\n",
      "step 1\n",
      "  x: [array([   47,  3542, 17584,  7684,   125,  7063,   248,    12, 18235,\n",
      "         136,     1, 11944, 15565,  3539,    34,  1623,  3301,  1953,\n",
      "         244,    12,   576, 18372,    14,    85,    86,   292,  2978,\n",
      "        1804,   614,    34,     1,  1204,     9,  1690,  1059,    12,\n",
      "        9937,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)\n",
      " array([17213, 10662,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)]\n",
      "  y: [array([0, 1]) array([0, 1])]\n",
      "step 2\n",
      "  x: [array([    1,   396,    80,   125,   592,    17, 11436,   156,     1,\n",
      "        1827,    13,    64, 11437,   236,   396, 11438,    12,   628,\n",
      "         456,  2588,   230, 11439,   121,   917,   115,    21,   236,\n",
      "        7858, 11440,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)\n",
      " array([   17,  5406, 14001,    59,  2574,  3924,    12,  6762,   250,\n",
      "           1,   740,  5675,    12,    59,  3902,  2475,    12,  8700,\n",
      "         250,     1, 14002,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0], dtype=int64)]\n",
      "  y: [array([1, 0]) array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# 合并到一起，一个元素代表一个样本\n",
    "test_dataset = list(zip(x_test, y_test))\n",
    "test_batch_iter = batch_iter(data=test_dataset, batch_size=2, num_epochs=1, shuffle=False)\n",
    "for step, batch in enumerate(test_batch_iter):\n",
    "    print('step', step)\n",
    "    print('  x:', batch[:,0])\n",
    "    print('  y:', batch[:,1])\n",
    "    if step == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "  x: <class 'numpy.ndarray'>\n",
      "  y: [array([1, 0]) array([1, 0]) array([1, 0]) array([0, 1]) array([0, 1])]\n",
      "  y: [1 0]\n",
      "step 1\n",
      "  x: <class 'numpy.ndarray'>\n",
      "  y: [array([1, 0]) array([1, 0]) array([0, 1]) array([0, 1]) array([0, 1])]\n",
      "  y: [1 0]\n",
      "step 2\n",
      "  x: <class 'numpy.ndarray'>\n",
      "  y: [array([0, 1]) array([0, 1]) array([0, 1]) array([0, 1]) array([0, 1])]\n",
      "  y: [0 1]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(test_batch_iter):\n",
    "    print('step', step)\n",
    "    print('  x:', type(batch[:,0]))\n",
    "    print('  y:', batch[:,1])\n",
    "    print('  y:', np.array(batch[:,1][0]))\n",
    "    if step == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "  x: <class 'tuple'>\n",
      "  y: [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "step 1\n",
      "  x: <class 'tuple'>\n",
      "  y: [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "step 2\n",
      "  x: <class 'tuple'>\n",
      "  y: [[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 合并到一起，一个元素代表一个样本\n",
    "test_dataset = list(zip(x_test, y_test))\n",
    "test_batch_iter = batch_iter(data=test_dataset, batch_size=5, num_epochs=1, shuffle=False)\n",
    "for step, batch in enumerate(test_batch_iter):\n",
    "    batch_xs, batch_ys = zip(*batch)\n",
    "    print('step', step)\n",
    "    print('  x:', type(batch_xs))\n",
    "    print('  y:', np.array(batch_ys))\n",
    "    if step == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Tensor(\"input_x_32:0\", shape=(2, 100), dtype=int32)\n",
      "## Tensor(\"embedding_32/embedding_lookup/Identity:0\", shape=(2, 100, 256), dtype=float32)\n",
      "## Tensor(\"embedding_32/ExpandDims:0\", shape=(2, 100, 256, 1), dtype=float32)\n",
      "## Tensor(\"conv_1_28/conv_1:0\", shape=(2, 98, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_1_28/maxpool_1:0\", shape=(2, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_1_28/Reshape:0\", shape=(2, 64), dtype=float32)\n",
      "## Tensor(\"conv_2_28/conv_2:0\", shape=(2, 97, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_2_28/maxpool_2:0\", shape=(2, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_2_28/Reshape:0\", shape=(2, 64), dtype=float32)\n",
      "## Tensor(\"conv_3_28/conv_3:0\", shape=(2, 96, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_3_28/maxpool_3:0\", shape=(2, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_3_28/Reshape:0\", shape=(2, 64), dtype=float32)\n",
      "## fc1 inputs: Tensor(\"concat_15:0\", shape=(2, 192), dtype=float32)\n",
      "fc1 input units: 192\n",
      "## fc1 outputs: Tensor(\"fc1_14/BiasAdd:0\", shape=(2, 3), dtype=float32)\n",
      "trainable_variables:\n",
      "<tf.Variable 'embedding_32/embedding_matrix:0' shape=(10000, 256) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1_28/kernel:0' shape=(3, 256, 1, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1_28/bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2_28/kernel:0' shape=(4, 256, 1, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2_28/bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'conv_3_28/kernel:0' shape=(5, 256, 1, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_3_28/bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'fc1_14/kernel:0' shape=(192, 3) dtype=float32_ref>\n",
      "<tf.Variable 'fc1_14/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.weights = []   # trainable_variables \n",
    "        \n",
    "    def __call__(self, inputs, keep_drop=1.0):\n",
    "        \"\"\" inputs 输入数据的形状为:\n",
    "        tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
    "        [\n",
    "            [词索引, 词索引, ....],  # 代表一个句子\n",
    "            [词索引, 词索引, ....],  # 代表一个句子\n",
    "            [],\n",
    "            ..\n",
    "        ]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('embedding'):\n",
    "            embedding = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], minval=-1.0, maxval=1.0), \n",
    "                                         name='embedding_matrix')\n",
    "            # 词索引 -> 词向量, 假设 sequence_length=100, embedding_size=256\n",
    "            # (None, 100) -> (None, 100, 256)\n",
    "            inputs_embedded = tf.nn.embedding_lookup(embedding, inputs)\n",
    "            # 转化为当通道图像的二维卷积   \n",
    "            # (None, 100, 256) -> (None, 100, 256, 1)\n",
    "            inputs_embedded_expanded = tf.expand_dims(inputs_embedded, -1)\n",
    "            print('##', inputs)\n",
    "            print('##', inputs_embedded)\n",
    "            print('##', inputs_embedded_expanded)\n",
    "            # 记录词向量矩阵\n",
    "            self.weights.append(embedding)\n",
    "            \n",
    "            \n",
    "        flattened_pooled_outputs = []    \n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            conv_op_name = 'conv_%s' % (i + 1)\n",
    "            with tf.name_scope(conv_op_name):\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='kernel')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=(self.num_filters,)), name='bias')\n",
    "                # 卷积操作，假设 filter_size=3, num_filters=64\n",
    "                # (None, 100, 256, 1) -> (None, 98, 1, 64)\n",
    "                conv_out = tf.nn.conv2d(inputs_embedded_expanded, W, strides=[1,1,1,1], padding='VALID', name=conv_op_name)\n",
    "                print('##', conv_out)\n",
    "                # 加偏置并激活\n",
    "                conv_out = tf.nn.relu(tf.nn.bias_add(conv_out, b))\n",
    "                # 记录卷积核及偏置\n",
    "                self.weights.append(W)\n",
    "                self.weights.append(b)\n",
    "            \n",
    "            maxpool_op_name = 'maxpool_%s' % (i + 1)\n",
    "            with tf.name_scope(maxpool_op_name):\n",
    "                pool_height_size = self.sequence_length - filter_size + 1\n",
    "                pool_width = 1\n",
    "                # 池化操作\n",
    "                # (None, 98, 1, 64) -> (None, 1, 1, 64)\n",
    "                pooled_out = tf.nn.max_pool(conv_out, ksize=[1, pool_height_size, pool_width, 1], strides=[1,1,1,1], \n",
    "                                          padding='VALID', name=maxpool_op_name)\n",
    "                print('##', pooled_out)\n",
    "                # (None, 1, 1, 64) -> (None, 64)\n",
    "                flattened_pooled_out = tf.reshape(pooled_out, shape=(-1, self.num_filters))\n",
    "                print(\"** Flaten:\", flattened_pooled_out)\n",
    "                flattened_pooled_outputs.append(flattened_pooled_out)\n",
    "                \n",
    "                \n",
    "        \n",
    "        # 把所有卷积池化后的结构拼接为一个长的向量作为全连接层的输入\n",
    "        fc1_in = tf.concat(flattened_pooled_outputs, axis=1)\n",
    "        print(\"## fc1 inputs:\", fc1_in)\n",
    "        \n",
    "        with tf.name_scope('fc1'):\n",
    "            input_units = fc1_in.shape[1].value\n",
    "            print(\"fc1 input units:\", input_units)\n",
    "            W = tf.Variable(tf.truncated_normal((input_units, self.num_classes), stddev=0.1), name='kernel')\n",
    "            b = tf.Variable(tf.constant(0.1, shape=(self.num_classes,)), name='bias')\n",
    "            fc1_out = tf.nn.bias_add(tf.matmul(fc1_in, W), b) \n",
    "            print(\"## fc1 outputs:\", fc1_out)\n",
    "            self.weights.append(W)\n",
    "            self.weights.append(b)\n",
    "        # 记录模型的输出 Tensor, 即各类别的打分数据，在调用处，用于优化损失\n",
    "        self.output = fc1_out\n",
    "        return self.output         \n",
    "\n",
    "\n",
    "text_cnn = TextCNN(sequence_length=100, num_classes=3, \n",
    "                   vocab_size=10000, embedding_size=256, \n",
    "                   filter_sizes=[3,4,5], num_filters=64)\n",
    "\n",
    "x = tf.placeholder(tf.int32, [2, 100], name='input_x')\n",
    "output = text_cnn(x)\n",
    "\n",
    "print(\"trainable_variables:\")\n",
    "for weight in text_cnn.weights:\n",
    "    print(weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Tensor(\"input_x_36:0\", shape=(?, 56), dtype=int32)\n",
      "## Tensor(\"embedding_36/embedding_lookup/Identity:0\", shape=(?, 56, 256), dtype=float32)\n",
      "## Tensor(\"embedding_36/ExpandDims:0\", shape=(?, 56, 256, 1), dtype=float32)\n",
      "## Tensor(\"conv_1_32/conv_1:0\", shape=(?, 54, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_1_32/maxpool_1:0\", shape=(?, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_1_32/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "## Tensor(\"conv_2_32/conv_2:0\", shape=(?, 53, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_2_32/maxpool_2:0\", shape=(?, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_2_32/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "## Tensor(\"conv_3_32/conv_3:0\", shape=(?, 52, 1, 64), dtype=float32)\n",
      "## Tensor(\"maxpool_3_32/maxpool_3:0\", shape=(?, 1, 1, 64), dtype=float32)\n",
      "** Flaten: Tensor(\"maxpool_3_32/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "## fc1 inputs: Tensor(\"concat_19:0\", shape=(?, 192), dtype=float32)\n",
      "fc1 input units: 192\n",
      "## fc1 outputs: Tensor(\"fc1_18/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = max_document_length\n",
    "num_classes = 2\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "embedding_size = 256\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 64\n",
    "\n",
    "\n",
    "text_cnn = TextCNN(sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters)\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "\n",
    "\n",
    "output = text_cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 - aac: 0.0000, loss: 1.7442, test acc: 0.4831, test loss: 3.5999\n"
     ]
    }
   ],
   "source": [
    "def train(output, x, y_):\n",
    "    # 交叉熵损失\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=output))\n",
    "    # 选择梯度优化算法\n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    correction_predictions = tf.equal(tf.argmax(output, axis=1), tf.argmax(y_, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correction_predictions, dtype=tf.float32))\n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        batch_size = 128\n",
    "        num_steps = 1000\n",
    "        eval_per_steps = 30\n",
    "        \n",
    "        train_dataset = list(zip(x_train, y_train))\n",
    "        test_dataset = list(zip(x_test, y_test))\n",
    "          \n",
    "        train_batch_iter = batch_iter(data=train_dataset, batch_size=50, num_epochs=1, shuffle=False)\n",
    "        test_batch_iter = batch_iter(data=test_dataset, batch_size=50, num_epochs=1, shuffle=False)\n",
    "        \n",
    "        for step, batch in enumerate(train_batch_iter):\n",
    "            batch_xs, batch_ys = zip(*batch)\n",
    "                \n",
    "            _, _train_loss = sess.run([train_step, loss],\n",
    "                                      feed_dict={ x: batch_xs, y_: batch_ys })\n",
    "\n",
    "            # 每隔100步计算一下模型的准确度\n",
    "            if step % eval_per_steps == 0:\n",
    "                _train_acc, _train_loss = sess.run([accuracy, loss],\n",
    "                                                   feed_dict={ x: batch_xs, y_: batch_ys })\n",
    "                _test_loss_total = 0.0\n",
    "                _test_acc_total = 0.0\n",
    "                \n",
    "                for i, batch in enumerate(test_batch_iter):\n",
    "                    batch_xs, batch_ys = zip(*batch)\n",
    "                    # 获取一批测试数据，计算准确度\n",
    "                    _test_acc, _test_loss = sess.run([accuracy, loss],\n",
    "                                                     feed_dict={ x: batch_xs, y_: batch_ys })\n",
    "                    _test_loss_total += _test_loss\n",
    "                    _test_acc_total += _test_acc\n",
    "                    \n",
    "                print('step {} - aac: {:.4f}, loss: {:.4f}, test acc: {:.4f}, test loss: {:.4f}'.format(\n",
    "                        step, _train_acc, _train_loss, _test_acc_total / (i+1), _test_loss_total / (i+1)))\n",
    "\n",
    "\n",
    "train(output, x, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal:0' shape=(?,) dtype=bool>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf1x': conda)",
   "language": "python",
   "name": "python361064bittf1xconda7075000cacf94cec96f83fcd9f204e9b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
